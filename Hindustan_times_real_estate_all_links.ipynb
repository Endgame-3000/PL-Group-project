{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=requests.get(\"https://www.hindustantimes.com/real-estate/\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=bs(content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links={}\n",
    "index=1\n",
    "f=open('links.text','w')\n",
    "for i in range(1,15):\n",
    "    newlink=requests.get('https://www.hindustantimes.com/real-estate/page/?pageno='+str(i))\n",
    "    l_soup=bs(newlink.text,\"html.parser\")\n",
    "    for link in l_soup.find_all('a'):\n",
    "        if (\"https://www.hindustantimes.com/real-estate\" in link['href']):\n",
    "            f.write('\\n Source :Hindustan Times Newspaper'+'\\n'+link.get('href'))\n",
    "            links[index]=(link.get('href'))\n",
    "            index+=1\n",
    "with open('links.json','w') as l:\n",
    "    json.dump(links,l,indent='\\n Source:Hindustan Times\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[]\n",
    "for link in soup.find_all('a'):\n",
    "    data={}\n",
    "    if (\"https://www.hindustantimes.com/real-estate/\" in link['href']):\n",
    "        title_link=requests.get(link.get('href'))\n",
    "        url=link.get('href')\n",
    "        urlinfo=al(url)\n",
    "        urlinfo.download()\n",
    "        urlinfo.parse()\n",
    "        rawsoup=bs(title_link.text,'html.parser')\n",
    "        data['Title']=(rawsoup.find('title').text)\n",
    "        data['Source']='Hindustan Times'\n",
    "        data['Link']=link.get('href')\n",
    "        data['Content']=urlinfo.text[49:]\n",
    "        data['Image Link']=urlinfo.top_image\n",
    "        for date in rawsoup.find_all('span',{'class':\"text-dt\"}):\n",
    "            if 'First Published:' in date.text:\n",
    "                data['Published_Date']=(date.text[27:])\n",
    "        data['Related News']=rawsoup.find('a',attrs={'class':'wclink2'}).get('href')\n",
    "        data['Author']=rawsoup.find('span',attrs={'class':'author'}).text\n",
    "        \n",
    "\n",
    "    articles.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles=articles[52:97]\n",
    "with open ('articles.json','w') as a:\n",
    "    json.dump(new_articles,a,indent='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
